.
├── 0.编程基础  
│   ├── 7款优秀Vim插件帮你打造完美IDE  
│   ├── All in Linux：一个算法工程师的IDE断奶之路  
│   ├── Git从入门到进阶，你想要的全在这里  
│   ├── 一份北大信科内部流传的 “CS 自救指南”（无广推荐）  
│   └── 算法工程师的效率神器——vim篇  
├── 1.机器学习基础  

│   ├── 0.数学基础

│   │   ├── 线性代数应该这样讲-三--向量2范数与模型泛化
│   │   ├── 线性代数应该这样讲-四--奇异值分解与主成分分析
│   │   ├── 线性代数应该这样讲（一）
│   │   └── 线性代数应该这样讲（二）
│   ├── 1.经典统计机器学习模型
│   │   ├── LightGBM最强解析，从算法原理到代码实现~
│   │   ├── 深入解析GBDT二分类算法（附代码实现）
│   │   ├── 机器学习系列-强填EM算法在理论与工程之间的鸿沟（上）
│   │   ├── 机器学习系列-强填EM算法在理论与工程之间的鸿沟（下）
│   │   ├── 深度前馈网络与Xavier初始化原理
│   │   ├── 从逻辑回归到最大熵模型
│   │   ├── 朴素贝叶斯与拣鱼的故事
│   │   ├── 从逻辑回归到受限玻尔兹曼机
│   │   ├── 逻辑回归与朴素贝叶斯的战争
│   │   ├── 从点到线：逻辑回归到条件随机场
│   │   └── 解开玻尔兹曼机的封印会发生什么？
│   ├── 怎样将Embedding融入传统机器学习框架？
│   ├── 持续学习简述
│   ├── 数据清洗指南
│   ├── 置信学习简述
│   ├── 入门指导手册1
│   ├── 入门指导手册2
│   ├── 入门指导手册3
│   ├── 类别不均衡问题
│   ├── 为什么回归问题用MSE？
│   ├── 强化学习扫盲贴：从Q-learning到DQN
│   ├── 在错误的数据上，刷到 SOTA 又有什么意义？
│   ├── 机器学习模型可解释性
│   └── 为什么搜索与推荐场景用AUC评价模型好坏？
├── 2.深度学习基础、前沿与实战技巧
│   ├── 基础篇
│   │   ├── 深度解析LSTM神经网络的设计原理
│   │   ├── 训练神经网络时如何确定batch的大小？
│   │   ├── 不要再纠结卷积的公式啦！0公式深度解析全连接前馈网络与卷积神经网络
│   │   ├── 你的模型真的陷入局部最优点了吗？
│   │   └── 从前馈到反馈：解析循环神经网络（RNN）及其tricks
│   ├── 技巧篇
│   │   ├── All in Linux：一个算法工程师的IDE断奶之路
│   │   ├── BERT重计算：用22.5%的训练时间节省5倍的显存开销（附代码）
│   │   ├── 训练效率低？GPU利用率上不去？快来看看别人家的tricks吧～
│   │   ├── 谈谈怎样提高炼丹手速
│   │   ├── 算法工程师的效率神器——vim篇
│   │   ├── 万万没想到，我的炼丹炉玩坏了
│   │   ├── 显存不够，如何训练大型神经网络？
│   │   ├── 模型训练太慢？显存不够用？这个算法让你的GPU老树开新花
│   │   └── 别再喊我调参侠！夕小瑶“科学炼丹”手册了解一下
│   └── 学术前沿
│       ├── AdaX：一个比Adam更优秀，带”长期记忆“的优化器
│       ├── ICLR2020---如何判断两个神经网络学到的知识是否一致
│       ├── ICLR2020满分论文 - 为什么梯度裁剪能加速模型训练？
│       ├── ICML2021 | Self-Tuning- 如何减少对标记数据的需求？  
│       ├── NYU & Google- 知识蒸馏无处不在，但它真的有用吗？  
│       ├── 你的 GNN，可能 99% 的参数都是冗余的  
│       ├── 可交互的 Attention 可视化工具！我的Transformer可解释性有救了？
│       ├── 硬核推导Google AdaFactor：一个省显存的宝藏优化器
│       ├── 负采样，yyds！ 
│       ├── 年末回顾：2021年 AI 领域十大研究趋势及必读论文  
│       ├── 一时学习一时爽，_持续学习_持续爽
│       ├── 一训练就显存爆炸？Facebook 推出 8 比特优化器，两行代码拯救你的显存！  
│       ├── 深度学习，路在何方？  
│       ├── 在错误的数据上，刷到 SOTA 又有什么意义？  
│       ├── 大模型炼丹无从下手？谷歌、OpenAI烧了几百万刀，总结出这些方法论…  
│       ├── 高效利用无标注数据：自监督学习简述
│       ├── 我拿模型当朋友，模型却想泄漏我的隐私？
│       ├── 我删了这些训练数据…模型反而表现更好了！？  
│       ├── 恕我直言，很多小样本学习的工作就是不切实际的
│       ├── 恕我直言，你的实验结论可能严重依赖随机数种子！  
│       ├── 别让数据坑了你！用置信学习找出错误标注（附开源实现）
│       └── 谷歌重磅：可以优化自己的优化器！手动调参或将成为历史！？
├── 3.自然语言处理与知识图谱
│   ├── 2020学术前沿
│   │   ├── ACL20 - 让笨重的BERT问答匹配模型变快！
│   │   ├── ACL2020 - 线上搜索结果大幅提升！亚马逊提出对抗式query-doc相关性模型
│   │   ├── ACL2020---FastBERT：放飞BERT的推理速度
│   │   ├── ACL2020---基于Knowledge-Embedding的多跳知识图谱问答
│   │   ├── ACL2020---对话数据集Mutual：论对话逻辑，BERT还差的很远
│   │   ├── FLAT：中文NER屠榜之作！
│   │   ├── GPT-3诞生，Finetune也不再必要了！NLP领域又一核弹！
│   │   ├── Google - 突破瓶颈，打造更强大的Transformer
│   │   ├── LayerNorm是Transformer的最优解吗？
│   │   ├── Transformer哪家强？Google爸爸辨优良！
│   │   ├── 当NLPer爱上CV：后BERT时代生存指南之VL-BERT篇
│   │   ├── 吊打BERT-Large的小型预训练模型ELECTRA终于开源！真相却让人---
│   │   ├── 万能的BERT连文本纠错也不放过
│   │   ├── 如何让BERT拥有视觉感知能力？两种方式将视频信息注入BERT
│   │   ├── 还在用[CLS]？从BERT得到最强句子Embedding的打开方式！
│   │   ├── 别再蒸馏3层BERT了！变矮又能变瘦的DynaBERT了解一下
│   │   ├── 卖萌屋上线Arxiv论文速刷神器，直达学术最前沿！
│   │   ├── 告别自注意力，谷歌为Transformer打造新内核Synthesizer
│   │   ├── 无需人工！无需训练！构建知识图谱 BERT一下就行了！
│   │   ├── 如何优雅地编码文本中的位置信息？三种positioanl encoding方法简述
│   │   └── 陈丹琦“简单到令人沮丧”的屠榜之作：关系抽取新SOTA！
│   ├── 2021-2022.3学术前沿
│   │   ├── 1000层的Transformer，诞生了！ 
│   │   ├── AllenAI | 用GPT-3帮助增建数据，NLI任务直接提升十个点！？  .webarchive
│   │   ├── AllenAI 发布万能问答系统 MACAW！各类题型样样精通，性能大幅超越 GPT-3！  
│   │   ├── Fine-tune之后的NLP新范式：Prompt越来越火，CMU华人博士后出了篇综述文章  
│   │   ├── Google Research新成果，让表格理解和检索更上一层楼！
│   │   ├── Google掀桌了，GLUE基准的时代终于过去了？  
│   │   ├── Prompt tuning新工作，五个参数解决下游任务 fine-tuning  
│   │   ├── Transformer太大了，我要把它微调成RNN
│   │   ├── 从 ACL’22 投稿情况，速览当下 NLP 研究热点！  
│   │   ├── 以4%参数量比肩GPT-3！Deepmind 发布检索型 LM，或将成为 LM 发展新趋势！？  
│   │   ├── 中文BERT上分新技巧，多粒度信息来帮忙
│   │   ├── 迁移Prompt–解决Prompt Tuning三大问题！  
│   │   ├── 别再Prompt了！谷歌提出tuning新方法，强力释放GPT-3潜力！  
│   │   ├── 超硬核 ICML’21 _ 如何使自然语言生成提速五倍，且显存占用减低99%
│   │   ├── 谁说发 paper 一定要追快打新？2021年，研究 word2vec 也能中顶会！  
│   │   ├── 清华提出LogME，无需微调就能衡量预训练模型的下游任务表现！  
│   │   ├── 用多模态信息做 prompt，解锁 GPT 新玩法  
│   │   ├── 如何提升大规模Transformer的训练效果？Primer给出答案  
│   │   ├── 发现一篇专门吐槽 NLP 内卷现状的 ACL 论文 ...  
│   │   ├── 软硬兼施极限轻量BERT！能比ALBERT再轻13倍？！
│   │   ├── 打破情感分类准确率 80 分天花板！更加充分的知识图谱结合范式  
│   │   ├── 分类问题后处理技巧CAN，近乎零成本获取效果提升
│   │   ├── 别再双塔了！谷歌提出DSI索引，检索效果吊打双塔，零样本超BM25！
│   │   ├── 丹琦女神的对比学习新SOTA，在中文表现如何？我们补充实验后，惊了！
│   │   ├── 这篇论文提出了一个文本_-_知识图谱的格式转换器.._
│   │   ├── 把数据集刷穿是什么体验？MetaQA已100%准确率
│   │   ├── 预训练卷不动，可以卷输入预处理啊！  
│   │   ├── 对比学习有多火？文本聚类都被刷爆了…
│   │   ├── 丹琦女神新作：对比学习，简单到只需要Dropout两下
│   │   └── 加了元学习之后，少样本学习竟然可以变得这么简单！  
│   ├── 基础知识
│   │   ├── 45个小众而实用的NLP开源字典和工具
│   │   ├── NLP-Subword三大算法原理：BPE、WordPiece、ULM
│   │   ├── NLP最佳入门与提升路线
│   │   ├── NLP的游戏规则从此改写？从word2vec,-ELMo到BERT
│   │   ├── Step-by-step-to-Transformer：深入解析工作原理（以Pytorch机器翻译为例）
│   │   ├── 那些击溃了所有NLP系统的样本
│   │   ├── 如何打造高质量的NLP数据集
│   │   ├── 文本分类问题不需要ResNet？小夕解析DPCNN设计原理（上）
   │   ├── 文本分类问题不需要ResNet？小夕解析DPCNN设计原理（下）
│   │   ├── 搜索引擎核心技术与算法-——-倒排索引初体验
│   │   ├── 斯坦福大学最甜网剧：知识图谱CS520面向大众开放啦！
│   │   ├── 如何优雅地编码文本中的位置信息？三种positioanl encoding方法简述的副本
│   │   ├── 中文分词的古今中外，你想知道的都在这里
│   │   ├── 文本分类有哪些论文中很少提及却对性能有重要影响的tricks？
│   │   └── 史上最可爱的关系抽取指南？从一条规则到十个开源项目
│   ├── 子方向综述
│   │   ├── 2202年了，“小样本”还值得卷吗？  
│   │   ├── CMU & MILA & 谷歌 _ 三家巨头发布史上最干文本增强总结
│   │   ├── NLP数据增强方法综述：EDA、BT、MixMatch、UDA
│   │   ├── NLP中的少样本困境问题探究
│   │   ├── NLP进入预训练模型时代：从word2vec,ELMo到BERT
│   │   ├── NLP哪个细分方向最具社会价值？
│   │   ├── 后BERT时代：15个预训练模型对比分析与关键点探究
│   │   ├── 打破BERT天花板：11种花式炼丹术刷爆NLP分类SOTA！
│   │   ├── 超一流 - 从XLNet的多流机制看最新预训练模型的研究进展
│   │   ├── 如何提高NLP模型鲁棒性和泛化能力？对抗训练论文综述
│   │   ├── 一文跟进Prompt进展！综述+15篇最新论文逐一梳理  
│   │   ├── 搜索中的Query理解及应用
│   │   ├── 工业界求解NER问题的12条黄金法则
│   │   ├── 对比学习综述
│   │   ├── 从零构建知识图谱
│   │   ├── 格局打开，带你解锁 prompt 的花式用法  
│   │   ├── 对话系统的设计艺术
│   │   ├── 文本对抗攻击入坑宝典
│   │   ├── 多轮对话与检索式聊天机器人(chatbot)综述
│   │   ├── 文本匹配相关方向打卡点总结
│   │   ├── 文本生成评价指标的进化与推翻
│   │   ├── 限定域文本语料的短语挖掘综述
│   │   ├── 任务完成型对话之对话状态追踪DST综述
│   │   ├── 基于知识图谱的篇章标签生成综述
│   │   ├── 智能问答系统与机器阅读理解分方向综述
│   │   ├── 预训练模型关键问题梳理与面试必备高频FAQ
│   │   ├── 中文分词的古今中外，你想知道的都在这里
│   │   └── 一人之力，刷爆三路榜单！信息抽取竞赛夺冠经验分享
│   └── 写了一篇关于 NLP 综述的综述！
├── 4.计算机视觉与跨模态
│   ├── ACL'21 _ 多模态数值推理新挑战，让 AI 学解几何题
│   ├── ACL’21 | 对话系统也要进军多模态了！  
│   ├── Allen AI提出MERLOT，视频理解领域新SOTA！
│   ├── Facebook 推出多模态通用模型 FLAVA，吊打 CLIP 平均十个点！  
│   ├── GAN 的内在漏洞！只看眼睛就能找出虚拟人脸？
│   ├── MSRA-万字综述 直击多模态文档理解  
│   ├── Meta AI 发布 data2vec！统一模态的新里程碑！  
│   ├── 吊打BERT、GPT、DALL·E，跨模态榜单新霸主诞生！
│   ├── 屠榜CV还不是这篇论文的终极目标，它更大的目标其实是……
│   ├── 刷新SOTA！Salesforce提出跨模态对比学习新方法，仅需4M图像数据！  
│   ├── 谷歌：CNN击败Transformer，有望成为预训练界新霸主！LeCun却沉默了.._
│   ├── 图灵奖大佬 Lecun 发表对比学习新作，比 SimCLR 更好用！  
│   ├── 图灵奖大佬+谷歌团队，为通用人工智能背书！CV 任务也能用 LM 建模！  
│   ├── 再介绍一篇最新的Contrastive Self-supervised Learning综述论文  
│   ├── 谷歌：一篇论文，让研究者吃我三份安利
│   ├── 惊呆！不用一张图片，却训出个图像识别SOTA？
│   ├── 视觉增强词向量：我是词向量，我开眼了！
│   ├── 多模态为什么比单模态好？第一份严谨证明来了！  
│   ├── 多模态为什么比单模态好？第一份严谨证明来了！
│   └── 对比学习效果差？谷歌提出弱语义负样本，有效学习高级特征！
├── 5.算法岗求职经验技巧
│   ├── 13个offer，8家SSP，谈谈我的秋招经验
│   ├── 6 年大厂面试官，谈谈我对算法岗面试的一些看法  
│   ├── Google、MS和BAT教给我的面试真谛
│   ├── 工作6年，谈谈我对“算法岗”的理解
│   ├── 大厂生存36计
│   ├── 面试必备基础知识
│   │   ├── 算法与数据结构--空间复杂度O-1-遍历树
│   │   ├── 「小公式」平均数与级数
│   │   ├── 算法工程师思维导图—深度学习篇
│   │   ├── 「小算法」回文数与数值合法性检验
│   │   ├── 算法工程师思维导图—数据结构与算法
│   │   ├── 算法工程师思维导图—统计机器学习篇
│   │   ├── 预训练模型关键问题梳理与面试必备高频FAQ
│   │   └── 卖萌屋算法岗面试手册上线！通往面试自由之路
│   ├── 一个接了等于自杀的高薪offer  
│   ├── 别再搜集面经啦！小夕教你斩下NLP算法岗offer！
│   ├── 在大厂和小厂做算法有什么不同？
│   └── 拒绝跟风，谈谈几种算法岗的区别和体验
├── 6.其他文章（搞笑、鬼畜、吐槽、科研技巧等）
│   ├── 11 个好用的科研工具推荐！工作效率提升 max！  
│   ├── 在斯坦福，做 Manning 的 phd 要有多强？  
│   ├── 在斯坦福，做 Manning 的 phd 要有多强？
│   ├── 无内鬼，来点ICML_ACL审稿人笑话
│   ├── 机器学习梗图大赏
│   ├── 两个月，刷了八千篇Arxiv，我发现……  
│   ├── 谢撩，人在斯坦福打SoTA
│   ├── 如果你跟夕小瑶恋爱了---（上）
│   ├── 如果你跟夕小瑶恋爱了---（下）
│   ├── 我在斯坦福做科研的碎碎念  
│   ├── 近期神奇机器学习应用大赏  
│   ├── 他与她，一个两年前的故事
│   ├── 如何优雅的追到女神夕小瑶
│   ├── 论文投稿新规则，不用跑出SOTA，还能“内定”发论文？！
│   ├── 万万没想到，我的炼丹炉玩坏了
│   ├── 我对你的爱，是只为你而留的神经元
│   ├── 盘点我跳过的科研天坑，进坑就是半年白干  
│   ├── 吐血整理：论文写作中注意这些细节，能显著提升成稿质量  
│   └── 一位老师，一位领导，一个让全体学生考上目标学校的故事
└── tree.txt

17 directories, 221 files




